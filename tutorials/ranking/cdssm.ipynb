{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../../MatchZoo-py/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matchzoo version 1.0\n",
      "`ranking_task` initialized with metrics [normalized_discounted_cumulative_gain@3(0.0), normalized_discounted_cumulative_gain@5(0.0), mean_average_precision(0.0)]\n",
      "data loading ...\n",
      "data loaded as `train_pack_raw` `dev_pack_raw` `test_pack_raw`\n"
     ]
    }
   ],
   "source": [
    "%run init.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = mz.models.CDSSM.get_default_preprocessor(\n",
    "    ngram_size = 3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing text_left with chain_transform of Tokenize => Lowercase => PuncRemoval: 100%|██████████| 2118/2118 [00:00<00:00, 4796.11it/s]\n",
      "Processing text_right with chain_transform of Tokenize => Lowercase => PuncRemoval: 100%|██████████| 18841/18841 [00:05<00:00, 3476.94it/s]\n",
      "Processing text_right with append: 100%|██████████| 18841/18841 [00:00<00:00, 819046.49it/s]\n",
      "Building FrequencyFilter from a datapack.: 100%|██████████| 18841/18841 [00:00<00:00, 115375.57it/s]\n",
      "Processing text_right with transform: 100%|██████████| 18841/18841 [00:00<00:00, 119855.07it/s]\n",
      "Processing text_left with extend: 100%|██████████| 2118/2118 [00:00<00:00, 600888.52it/s]\n",
      "Processing text_right with extend: 100%|██████████| 18841/18841 [00:00<00:00, 650935.58it/s]\n",
      "Building Vocabulary from a datapack.: 100%|██████████| 418401/418401 [00:00<00:00, 2305897.50it/s]\n",
      "Processing text_left with transform: 100%|██████████| 2118/2118 [00:00<00:00, 51641.26it/s]\n",
      "Processing text_right with transform: 100%|██████████| 18841/18841 [00:01<00:00, 12813.49it/s]\n",
      "Processing text_left with extend: 100%|██████████| 2118/2118 [00:00<00:00, 474218.54it/s]\n",
      "Processing text_right with extend: 100%|██████████| 18841/18841 [00:00<00:00, 362504.45it/s]\n",
      "Building Vocabulary from a datapack.: 100%|██████████| 2082963/2082963 [00:00<00:00, 2897885.52it/s]\n",
      "Processing text_left with chain_transform of Tokenize => Lowercase => PuncRemoval: 100%|██████████| 2118/2118 [00:00<00:00, 7561.20it/s]\n",
      "Processing text_right with chain_transform of Tokenize => Lowercase => PuncRemoval: 100%|██████████| 18841/18841 [00:04<00:00, 3925.81it/s]\n",
      "Processing text_right with transform: 100%|██████████| 18841/18841 [00:00<00:00, 108904.00it/s]\n",
      "Processing text_left with transform: 100%|██████████| 2118/2118 [00:00<00:00, 155573.11it/s]\n",
      "Processing text_right with transform: 100%|██████████| 18841/18841 [00:00<00:00, 101905.13it/s]\n",
      "Processing length_left with len: 100%|██████████| 2118/2118 [00:00<00:00, 630529.91it/s]\n",
      "Processing length_right with len: 100%|██████████| 18841/18841 [00:00<00:00, 712192.52it/s]\n",
      "Processing text_left with chain_transform of Tokenize => Lowercase => PuncRemoval: 100%|██████████| 122/122 [00:00<00:00, 7592.29it/s]\n",
      "Processing text_right with chain_transform of Tokenize => Lowercase => PuncRemoval: 100%|██████████| 1115/1115 [00:00<00:00, 3953.23it/s]\n",
      "Processing text_right with transform: 100%|██████████| 1115/1115 [00:00<00:00, 117309.21it/s]\n",
      "Processing text_left with transform: 100%|██████████| 122/122 [00:00<00:00, 106538.64it/s]\n",
      "Processing text_right with transform: 100%|██████████| 1115/1115 [00:00<00:00, 93409.68it/s]\n",
      "Processing length_left with len: 100%|██████████| 122/122 [00:00<00:00, 108023.03it/s]\n",
      "Processing length_right with len: 100%|██████████| 1115/1115 [00:00<00:00, 570880.00it/s]\n",
      "Processing text_left with chain_transform of Tokenize => Lowercase => PuncRemoval: 100%|██████████| 237/237 [00:00<00:00, 7611.70it/s]\n",
      "Processing text_right with chain_transform of Tokenize => Lowercase => PuncRemoval: 100%|██████████| 2300/2300 [00:00<00:00, 3960.70it/s]\n",
      "Processing text_right with transform: 100%|██████████| 2300/2300 [00:00<00:00, 122760.64it/s]\n",
      "Processing text_left with transform: 100%|██████████| 237/237 [00:00<00:00, 145669.70it/s]\n",
      "Processing text_right with transform: 100%|██████████| 2300/2300 [00:00<00:00, 102373.92it/s]\n",
      "Processing length_left with len: 100%|██████████| 237/237 [00:00<00:00, 155174.84it/s]\n",
      "Processing length_right with len: 100%|██████████| 2300/2300 [00:00<00:00, 643813.35it/s]\n"
     ]
    }
   ],
   "source": [
    "train_pack_processed = preprocessor.fit_transform(train_pack_raw)\n",
    "valid_pack_processed = preprocessor.transform(dev_pack_raw)\n",
    "test_pack_processed = preprocessor.transform(test_pack_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ngram_process_unit': <matchzoo.preprocessors.units.ngram_letter.NgramLetter at 0x7fce68035240>,\n",
       " 'filter_unit': <matchzoo.preprocessors.units.frequency_filter.FrequencyFilter at 0x7fce68035160>,\n",
       " 'vocab_unit': <matchzoo.preprocessors.units.vocabulary.Vocabulary at 0x7fce68123cf8>,\n",
       " 'vocab_size': 30058,\n",
       " 'embedding_input_dim': 30058,\n",
       " 'ngram_vocab_unit': <matchzoo.preprocessors.units.vocabulary.Vocabulary at 0x7fcd604f64e0>,\n",
       " 'ngram_vocab_size': 9654}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor.context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "triletter_callback = mz.dataloader.callbacks.Ngram(preprocessor, mode='sum')\n",
    "trainset = mz.dataloader.Dataset(\n",
    "    data_pack=train_pack_processed,\n",
    "    mode='pair',\n",
    "    num_dup=2,\n",
    "    num_neg=1,\n",
    "    callbacks=[triletter_callback]\n",
    ")\n",
    "testset = mz.dataloader.Dataset(\n",
    "    data_pack=test_pack_processed,\n",
    "    callbacks=[triletter_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding_callback = mz.models.CDSSM.get_default_padding_callback(\n",
    "    fixed_length_left=10,\n",
    "    fixed_length_right=100,\n",
    "    pad_word_value=0,\n",
    "    pad_word_mode='pre',\n",
    "    with_ngram=True,\n",
    "    fixed_ngram_length=preprocessor.context['ngram_vocab_size'],\n",
    "    pad_ngram_value=0,\n",
    "    pad_ngram_mode='pre'\n",
    ")\n",
    "\n",
    "trainloader = mz.dataloader.DataLoader(\n",
    "    dataset=trainset,\n",
    "    batch_size=20,\n",
    "    stage='train',\n",
    "    sort=False,\n",
    "    resample=True,\n",
    "    callback=padding_callback\n",
    ")\n",
    "testloader = mz.dataloader.DataLoader(\n",
    "    dataset=testset,\n",
    "    batch_size=20,\n",
    "    stage='dev',\n",
    "    sort=False,\n",
    "    callback=padding_callback\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CDSSM(\n",
      "  (net_left): Sequential(\n",
      "    (0): ConstantPad1d(padding=(0, 2), value=0)\n",
      "    (1): Conv1d(9654, 64, kernel_size=(3,), stride=(1,))\n",
      "    (2): Tanh()\n",
      "    (3): Dropout(p=0.8, inplace=False)\n",
      "    (4): AdaptiveMaxPool1d(output_size=1)\n",
      "    (5): Squeeze()\n",
      "    (6): Sequential(\n",
      "      (0): Sequential(\n",
      "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (1): Tanh()\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (1): Tanh()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (net_right): Sequential(\n",
      "    (0): ConstantPad1d(padding=(0, 2), value=0)\n",
      "    (1): Conv1d(9654, 64, kernel_size=(3,), stride=(1,))\n",
      "    (2): Tanh()\n",
      "    (3): Dropout(p=0.8, inplace=False)\n",
      "    (4): AdaptiveMaxPool1d(output_size=1)\n",
      "    (5): Squeeze()\n",
      "    (6): Sequential(\n",
      "      (0): Sequential(\n",
      "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (1): Tanh()\n",
      "      )\n",
      "      (1): Sequential(\n",
      "        (0): Linear(in_features=64, out_features=64, bias=True)\n",
      "        (1): Tanh()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (out): Linear(in_features=1, out_features=1, bias=True)\n",
      ")\n",
      "Trainable params:  3723906\n"
     ]
    }
   ],
   "source": [
    "model = mz.models.CDSSM()\n",
    "\n",
    "model.params['task'] = ranking_task\n",
    "model.params['vocab_size'] = preprocessor.context['ngram_vocab_size']\n",
    "model.params['filters'] = 64\n",
    "model.params['kernel_size'] = 3\n",
    "model.params['conv_activation_func'] = 'tanh'\n",
    "model.params['mlp_num_layers'] = 1\n",
    "model.params['mlp_num_units'] = 64\n",
    "model.params['mlp_num_fan_out'] = 64\n",
    "model.params['mlp_activation_func'] = 'tanh'\n",
    "model.params['dropout_rate'] = 0.8\n",
    "\n",
    "model.build()\n",
    "\n",
    "print(model)\n",
    "print('Trainable params: ', sum(p.numel() for p in model.parameters() if p.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adadelta(model.parameters())\n",
    "\n",
    "trainer = mz.trainers.Trainer(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    trainloader=trainloader,\n",
    "    validloader=testloader,\n",
    "    validate_interval=None,\n",
    "    epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f78ed1d1b5e24569b28810d0574abf43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=102), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
